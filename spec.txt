Crypto Market Dashboard Technical Specification
System Architecture (High Level)
Architecture Diagram: The system uses a client-side web application, a serverless backend, and a minimal database to automate data flow from external crypto APIs to the user interface. This high-level design ensures data retrieval and updates occur automatically in the backend (no manual intervention), while the frontend displays up-to-date information by querying the backend. Key components include:
Frontend (Client) – A web-based dashboard (could be a static single-page application) that presents crypto market data to users. It communicates with backend APIs to fetch data, rather than directly calling third-party services, for reliability and rate-limit protection.
Backend (Serverless) – A set of cloud functions (e.g. AWS Lambda or similar) that handle data fetching and API requests. These functions run on a schedule and on-demand: scheduled jobs pull fresh data from external crypto APIs and cache it, and on-demand functions serve this cached data to the frontend via HTTP endpoints . This serverless approach scales with usage and avoids maintaining servers.
Data Store – A minimal database or caching layer to store fetched data (e.g. a NoSQL table or in-memory cache). The backend writes the latest market data and possibly short historical time-series to the store. The frontend reads from this store via the backend API. Storing data ensures the dashboard can quickly serve data and handle brief third-party outages by using the last known values.
Data Flow: On a fixed interval (e.g. every few minutes), a scheduled backend job triggers data ingestion. The backend fetches current prices, volumes, and other metrics from the chosen crypto API, processes it (ensuring consistency), and updates the database . The frontend, when loaded or on a refresh interval, calls backend endpoints to retrieve the latest cached data and then renders it. This separation means the site updates itself without manual input after deployment.
Data Sources & Update Strategy
Data Requirements: The dashboard will display key market indicators: individual cryptocurrency prices, trading volumes, volatility metrics, correlation between major assets, and market-wide aggregates (e.g. total market cap and volume). To fulfill these, we will utilize a reliable public crypto data API and define how often each data type updates:
Market Prices & Volume (per coin): Use a public API like CoinGecko for real-time price data. CoinGecko’s API is free to use without an API key and provides price, market cap, 24h volume, and price change in a single call . For example, an endpoint like /simple/price or /coins/markets returns price, 24h volume, market cap, and 24h change for multiple coins. Updates should be frequent (e.g. every 1 to 5 minutes) to keep prices current. The backend will poll this API on schedule and update the cache.
Market-Wide Metrics: Leverage the API’s global market endpoint to get data such as total crypto market capitalization, total 24h trading volume, number of active cryptocurrencies, and dominance of top coins. For instance, CoinGecko offers a /global endpoint for aggregate data . This can be updated on a similar schedule (e.g. every 5 minutes) since global metrics shift with market activity.
Volatility Metrics: If the chosen API does not directly provide volatility, the system will compute it. A common approach is to calculate historical volatility as the standard deviation of daily returns over a recent period (e.g. 30 days). This requires historical price data which can be fetched via the API’s historical endpoints (e.g. CoinGecko’s /coin/market_chart for the last 30 days). The backend could calculate volatility daily or every few hours, since volatility doesn’t need real-time updates. (Alternatively, some analytics APIs provide historical volatility on request , but using our own calculation avoids extra dependencies.)
Correlation Data: To show correlations between major cryptocurrencies, the system will periodically compute a correlation coefficient (e.g. Pearson correlation) using historical price series. For example, it can fetch daily closing prices for the top 2–5 coins over the past N days and compute their correlation matrix. This can be done once per day or on-demand when the dashboard loads a correlation view, as these values change slowly. Some services offer ready-made crypto correlation matrices , but computing in-house using the price data from our main API ensures consistency. We will likely focus on a few key correlations (e.g. BTC–ETH, BTC–market, etc.) for simplicity.
API Provider Choice: CoinGecko is recommended due to its comprehensive free tier and no API key requirement . It offers real-time data, historical data, and global metrics, covering all our needs. By contrast, CoinMarketCap’s free tier requires registration and is quite limited in data and rate limits , which could hinder automation. Therefore, CoinGecko’s API will serve as the primary data source for all market data. (If needed in the future, alternate sources like CryptoCompare or Binance’s public data can supplement data or provide redundancy.)
Update Frequencies: Prices and volumes will update frequently (every few minutes) to capture market movements. Global metrics can update at the same interval since the API calls are lightweight. Volatility and correlations, being derived metrics, update less frequently (daily or a few times a day) and can be computed in background jobs. The update schedule will be configured in the backend (e.g. a cron expression for serverless functions) to run automatically, ensuring the dashboard stays current without manual input .
Rate Limiting & Caching: We will design updates to respect the API’s rate limits (CoinGecko allows a generous number of calls per minute). By caching results in our backend database and serving multiple users from that cache, we minimize direct API calls. If needed, the backend will stagger different data fetches (e.g. not pulling all coins every second, but rather a batch of data every few minutes). This strategy guarantees fresh data while avoiding hitting API limits or incurring costs.
Backend Implementation Plan
The backend will be implemented as a lightweight, serverless application focused on data retrieval, caching, and exposing data to the frontend. The implementation plan covers API design, background jobs, caching layers, and error handling:
Tech Stack & Hosting: Use a serverless platform (for example, AWS Lambda with API Gateway, or an alternative like Cloudflare Workers or Google Cloud Functions) for a low-cost, scalable backend. This avoids server maintenance and falls within free-tier limits (e.g. AWS Lambda’s free tier includes 1 million requests per month, and 25GB-month of DynamoDB storage , sufficient for our needs). A Node.js or Python runtime can be used to interact with the crypto API and database.
Endpoints (API Design): Define clear HTTP endpoints for the frontend to fetch data. Likely endpoints:
GET /api/summary – Returns aggregated data needed for the main dashboard in one payload. This may include an array of top coins with their price, volume, change, etc., plus global metrics and last update time. Combining data reduces the number of requests the client must make.
GET /api/coins/:id/history – (Optional) Returns historical price data for a specific coin (to draw charts or compute correlations on the fly). This would fetch from our database (or directly from the external API if we choose on-demand for charts).
GET /api/metrics/correlation – (Optional or for future) Returns pre-computed correlation values or a matrix for selected coins. This might be included in the summary or provided as a separate endpoint if the frontend has a dedicated correlation view.
These endpoints will be implemented as stateless functions that read from the cache/database. If the data is stale or missing, the function can trigger a refresh from the external API (or rely on the scheduled jobs having updated it recently).
Background Jobs: Use scheduled triggers to automate data ingestion. For AWS, we would use Amazon EventBridge (CloudWatch events) to invoke a Lambda function on a fixed schedule (e.g. every 5 minutes) . In the Serverless framework, this is configured in code (as shown in an example serverless.yml with a rate(5 minutes) schedule ). Each scheduled run will:
Call the external API endpoints (e.g. one call for global data, one call for a batch of coin prices/volumes).
Perform any needed processing (e.g. calculating volatility from the last 30 days of data, if not done separately).
Store the results in the database (upserting the latest values).
Log the outcome (success or errors).
Separately, a daily scheduled job can fetch historical prices and compute correlations, storing those results (or intermediate data) in the database for the correlation endpoint.
Caching & Performance: The backend will cache recent data in the database or an in-memory store to serve the frontend quickly. For instance, after each 5-minute data pull, the latest snapshot of coin metrics and global stats is stored. The API endpoints (/api/summary) then simply read this snapshot and return it, making the client response fast (no waiting on third-party API calls during page load). We may also implement an in-memory cache within the function invocation (if using a warm lambda or a global variable) to reduce database reads, but given the stateless nature of serverless, relying on a single source of truth in the DB is simpler. The cache will include a timestamp so the frontend can display last updated time and the backend can decide if it should refresh data out-of-band.
Error Handling & Reliability: The backend will be built to handle API failures gracefully:
If a scheduled data fetch fails (due to network issues or API downtime), the system will keep the previous data intact and log the error. The next schedule can retry. The frontend should still receive the last cached data (with an indication of its timestamp) rather than nothing.
The data fetch functions will include basic retry logic for transient failures (e.g. HTTP 502 or timeouts from the API). For example, if a price fetch fails, try again after a short delay, but limit retries to avoid infinite loops.
All exceptions in the serverless functions will be caught and logged. Monitoring can be set up (e.g. CloudWatch logs or a service like Dashbird) to alert if data hasn’t updated in a while.
The API responses to the client should include status info. For example, include the last_updated time for each portion of data. The frontend can use this to display a warning if data is stale (e.g. “Data last updated 30 minutes ago” if normally it updates every 5 minutes).
Security & Usage Limits: Since the backend only provides data and does not handle user accounts or sensitive transactions, security is mostly about preventing abuse. We will set reasonable rate limits on our endpoints (if the site is public) to prevent excessive traffic from exhausting our free tier or API limits. This can be done via the API gateway or within the function (tracking IP usage). Also, no user funds or keys are involved, so the scope of security is limited to protecting the API key (if any) and ensuring data integrity. (With CoinGecko, no API key is needed, simplifying this aspect.)
In summary, the backend will act as a data bridge: automatically collecting and storing crypto market data and exposing it via simple APIs. By using serverless functions and managed services, the solution remains low-cost and requires minimal manual maintenance once deployed.
Database Design (Minimal)
The database serves as a cache and historical storage for the market data. We aim for a minimal schema that supports the dashboard’s features without unnecessary complexity. A NoSQL document store or key-value database is suitable, as the data can be naturally grouped by coin or by data type. Using a managed cloud database with a free tier (like DynamoDB, MongoDB Atlas, or Cloudflare KV) is preferred for low cost. The minimal schema will include:
Coins Collection/Table: Stores the latest snapshot of metrics for each tracked cryptocurrency. Each item might contain fields such as:
coin_id (string or symbol, e.g. “bitcoin”),
name (e.g. “Bitcoin”),
price_usd,
24h_volume_usd,
market_cap_usd,
percent_change_24h,
last_updated (timestamp of data retrieval).
This allows quick lookup of current stats. We might store only the top N coins here (as needed for the dashboard).
Global Metrics: A record for market-wide data (could be a single document or a small table with a single entry). Fields:
total_market_cap_usd,
total_24h_volume_usd,
btc_dominance_percent (if provided or computed),
active_cryptocurrencies (count of coins),
last_updated.
Since there’s typically only one global record, it can be identified by a key like global_stats. This is updated each cycle.
Historical Data (Time-Series): To support volatility and correlation calculations, we will maintain a limited history of prices:
A PriceHistory collection keyed by coin and date (or a time-series table if available). Each entry: coin_id, date (or timestamp, e.g. daily at 00:00 UTC), price_usd. This can store daily closing prices (or average prices) for the past 30 or 60 days for selected coins. We only need a small window to calculate recent volatility and correlation. A possible key schema is composite (coin_id + date).
Optionally, we can maintain a Computed Metrics collection: e.g. store precomputed 30-day volatility for each coin and correlation between coins. For instance, a record like { "metric": "volatility_30d", "coin_id": "bitcoin", "value": 0.65, "last_updated": "2025-12-01" } could hold the latest volatility. Similarly, a correlation entry could be { "metric": "corr_30d", "pair": "btc_eth", "value": 0.8 }. However, these might be unnecessary if the backend computes on the fly and returns results directly. We would opt to compute on schedule and cache if performance is a concern.
Schema & Size Considerations: The amount of data is small. Even tracking 50 coins with a year of daily history is manageable (50 * 365 ≈ 18,250 records). The free tier of most databases can handle this volume easily (e.g. DynamoDB’s free 25 GB storage covers it). We will index the data by coin and date for quick retrieval. In DynamoDB terms, coin_id could be the partition key and date the sort key in a time-series table.
Time-Series Handling: We will design the historical table with a retention policy – e.g. only keep the last N days to limit growth. The daily scheduled job that fetches historical prices can also delete outdated entries beyond our window if needed. Alternatively, since CoinGecko’s API can provide recent historical data on request, we might forego storing extensive history and just fetch required historical data on-demand for calculations. But storing a rolling window in the DB provides faster access and allows computing metrics without hitting the external API each time.
Consistency: The database updates (writes) happen only from the backend jobs. Each scheduled fetch will overwrite the Coins and Global metrics with fresh data. These writes should ideally be atomic per collection (e.g. update all top coin entries in a batch, or replace a single JSON document containing an array of coins). This ensures the frontend doesn’t see partial updates. We might use transactions or upsert batches if the database supports it. For simplicity, one approach is to store all coin data in a single JSON blob (since it’s a small set) under a key like all_coins_snapshot. Then the scheduled job just replaces that document each time. The frontend endpoint can retrieve this blob. This reduces complexity at the cost of fine-grained access, which we likely don’t need.
No User Data: There is no need for user accounts or portfolios in the MVP, so the database contains only public market data. This keeps the schema minimal and avoids privacy concerns.
In summary, the database design is focused on caching latest data and storing short-term history for analytics. This minimal schema supports the dashboard’s features (like volatility and correlations) without heavy relational complexity.
Frontend Pages & Components
The frontend will be a single-page application (SPA) or dynamic webpage that presents the crypto market dashboard. It should be designed for clarity and real-time updates, using simple, responsive components. The main page (Dashboard) will include the following sections and UI elements:
Global Market Summary: A top bar or header section highlighting market-wide metrics. This might show total cryptocurrency market capitalization, total 24h trading volume, BTC dominance, and the number of active coins. These give the user a high-level context of the market’s size and activity. For example: “Total Market Cap: $1.2 Trillion, 24h Volume: $80 Billion, BTC Dominance: 45%”. This section also displays the last updated time for the data.
Key Assets Overview: A table or grid listing the top cryptocurrencies (by market cap, or a curated list). For each coin, display key data points: Price, 24h Change (% or ±), 24h Volume, and possibly Market Cap. This provides a snapshot of major coins at a glance . The change could be color-coded (e.g. green for positive, red for negative) for quick visual scanning. The list might be limited to top 10 or 20 assets in MVP to keep it concise.
Volatility Indicator: A component showing volatility metrics. This could be as simple as an extra column in the assets table (e.g. “30d Volatility”) or a separate small section. For instance, a “Volatility Index” panel could list the computed volatility percentage for the top coins (e.g. “BTC 30d Volatility: 5.2%”). If visualizing, a sparkline or mini chart could illustrate price fluctuations for each coin. However, for MVP a numeric value or rating (high/medium/low) of volatility suffices.
Correlation Matrix or Highlights: For correlation data, the UI might include a small matrix or a list of interesting correlations. For example, a 2D matrix of BTC, ETH, and a few others showing correlation coefficients. If space is an issue, we can instead list statements like “BTC–ETH correlation: 0.85 (over last 30 days)” or “BTC’s correlation with overall market: 0.9”. This informs users how similarly coins move. A simple table might have rows of coin pairs and their correlation. In MVP, we might focus on a single correlation (e.g. each coin vs Bitcoin) to avoid a large matrix UI.
Interactive Elements: The dashboard page will auto-refresh data in the background. We plan to use client-side polling: the frontend will call the /api/summary (or relevant endpoints) at a fixed interval (e.g. every 1 or 2 minutes) to fetch new data and update the UI seamlessly. This interval can be tuned or made adjustable if needed. Using polling is straightforward given our data update strategy; alternatively, if using websockets or SSE was feasible, that could push updates to the client, but a simple periodic fetch is sufficient for a few-minute granularity. The user can also manually refresh the page if desired, but the goal is that they shouldn’t need to.
User Experience Considerations:
The design will be responsive (mobile-friendly). Use a simple CSS framework or responsive design so that on smaller screens the table might scroll or stack nicely.
Emphasize clarity: the data should be clearly labeled and formatted (e.g. using commas for large numbers, showing currency symbols). Avoid clutter – focus on the requested metrics.
Show loading or update states subtly. For example, when new data is fetching, a spinner or a fade effect can indicate refresh, or the last-updated timestamp can update.
Error display: If the backend is unreachable or data is stale, show a message in the UI (e.g. “Data currently unavailable. Showing last known values.”) so the user is aware. This ties into the backend providing status.
Technology: The frontend can be implemented with any framework or just vanilla JS since it’s relatively simple. A popular choice is React (with a minimal setup) for ease of building components, but even a static HTML page that uses JavaScript fetch calls would work. Given that we might host it on a static hosting, a small React app bundled with Vite or CRA is fine. We will use fetch/AJAX to call the backend API. No heavy state management is required if the data is mostly fetched and displayed, but a library like Redux or context could help if the app grows (for MVP, likely not necessary).
Pages and Routing: For MVP, everything is on one page (Dashboard). We are not implementing multi-page navigation or detailed subpages. All components (global summary, coin list, etc.) are sections on the main dashboard. In the future, we could add a detail page per coin (with a route like /coin/bitcoin showing a price chart and more info), but the MVP defers that. So routing can be very simple or non-existent initially.
Refreshing Behavior: As noted, the page will auto-refresh certain data. Notably, if some data (like correlation) is only updated daily, we might not refresh that every minute. We will likely fetch the whole summary (including correlation if included) on the same interval for simplicity. The last-updated time display will reassure users that data is live. We should ensure the UI update is smooth (maybe using React state or updating the DOM without full page reload).
No User Interaction Required: The dashboard does not require user input to function (no login, no buttons to load data except possibly a refresh). It’s primarily a read-only display that updates itself. Users simply open it and observe the data. This keeps the interface straightforward and focused on information.
In summary, the frontend will consist of a single, dynamic dashboard page with components for global stats, a list of key coins with price/volume/change, and additional panels for volatility and correlation. The design will prioritize clarity, real-time feedback, and be implemented in a modern, lightweight way. By keeping the UI logic simple (just periodic data fetch and render), the development and maintenance are straightforward.
Step-by-Step Build Order
To implement the project in a clear progression, we break the work into phases and steps. Each step builds on the previous, from basic setup to a polished product:
Phase 1: Project Setup and Basic Data Display
Environment Setup: Choose the tech stack and set up the repository. For example, initialize a frontend project (plain HTML/JS or using React/Vue) and set up a backend project (e.g. an AWS Lambda using Serverless Framework or a simple Node/Express app for testing). Ensure you have API credentials if needed (CoinGecko doesn’t require one, simplifying this step). Verify you can deploy a “Hello World” for both frontend (e.g. served on GitHub Pages or locally) and backend (e.g. a test Lambda or local Express server).
Fetch Basic Data (Prototype): Write a small script or function to call the crypto API for a simple piece of data – for instance, fetch Bitcoin’s current price from CoinGecko’s API. Run it locally to verify you can retrieve data. This is just to validate connectivity and familiarize with the API response format.
Basic Frontend Display: Hard-code or use the fetched data to display something on the webpage. For example, show “Bitcoin price: $XYZ”. This could be done by having the frontend call the external API directly initially (for quick prototyping) or by copying a JSON response. The goal is to confirm the data can be shown in the UI.
Version Control & Hosting Setup: Put the code into version control (e.g. GitHub). Set up the free hosting for the frontend (e.g. GitHub Pages, Netlify, or Vercel) so that any push deploys the static site. For the backend, set up deployment as well (e.g. configure Serverless Framework for AWS or deploy a Cloud Function). At this stage, the backend might not do much yet, but setting up the pipeline early is useful.
Phase 2: Backend Development and Automated Data Ingestion
5. Implement Data Fetching Lambda/Function: Develop the backend function that will fetch all required data from the external API. Start with the essentials: get a list of top coins with price, volume, etc., and get global market stats. Parse the API responses and structure the data into our own format (JSON objects for coins and global metrics). Test this function locally or in a cloud sandbox, printing the output to ensure all fields are captured correctly.
6. Set Up the Database: Choose a minimal database solution and configure it. For example, create a DynamoDB table or a Firebase Firestore collection for the coin data. Write code in the backend function to upsert the fetched data into the database. For now, focus on current data (one entry per coin and one for global stats). After running the function, verify that the data is stored (e.g. query the DB manually or log the operations). Ensure that the database credentials or connection (if needed) are properly set up in the environment (for instance, IAM roles for Lambda to access DynamoDB, or API keys for a DBaaS).
7. Scheduled Updates: Configure an automatic trigger for the data-fetch function. In AWS, this means adding a schedule event (cron) so it runs every X minutes . In other setups, use a scheduler (GCP Cloud Scheduler, or if on a traditional server, a cron job). Deploy this configuration. Monitor that the function indeed runs on schedule and updates the DB. You can verify by checking logs or seeing timestamp changes in the stored data.
8. Backend API Endpoints: Now implement the on-demand API endpoints that the frontend will call. For example, create a GET /api/summary Lambda (or HTTP function) that reads from the database and returns the latest data in JSON. Test this by invoking the endpoint (e.g. via curl or Postman) and ensure it returns the expected JSON structure. At this point, you have a backend that continually collects data and can serve it to clients.
9. Connect Frontend to Backend API: Modify the frontend to load data from the new backend API instead of any placeholder. Using JavaScript fetch (or Axios, etc.), call /api/summary when the page loads. Populate the UI elements with the returned data (list of coins, etc.). This is where you actually implement the dynamic table of prices, volume, etc., looping through the data. Verify that when you run the full system (perhaps deploying backend and frontend), the frontend displays real live data from the backend. This effectively achieves a basic MVP where the dashboard shows current prices and market stats, auto-fed by the backend.
Phase 3: Implement Advanced Metrics (Volatility & Correlation) and Polishing
10. Historical Data & Volatility Calculation: Extend the backend data fetcher to also retrieve historical prices needed for volatility. For example, on one of the scheduled runs (maybe a daily job), call the API’s endpoint for the last 30 days of price data for each key coin. Store this in the database (PriceHistory). Then implement a function to calculate volatility (std dev of returns) for each coin and store that in a field or separate record. Alternatively, calculate on the fly in the endpoint. Ensure this calculation is correct by testing with sample data. Update the summary API to include a volatility metric for each coin (e.g. add volatility_30d to each coin’s data).
11. Correlation Calculation: Similar to above, fetch historical data for correlation (if not already available from the volatility step). Compute the correlation coefficients for the desired pairs. This could be done in a scheduled job (maybe the same daily job as volatility). Store the results in the DB. For simplicity, you might compute correlations of all top coins against Bitcoin or produce a small matrix for top 5 coins. Update or create an API endpoint to fetch this correlation data. Test the output (e.g. are values between -1 and 1, and do they make sense given recent market behavior).
12. Frontend for Volatility & Correlation: Enhance the frontend to display the new metrics. Add a column or tooltip for volatility in the coin table. Add a section for correlation (even if simple text or a small table). Ensure the data comes through from the API and renders correctly. This might involve adjusting how the frontend handles the JSON response. Keep the UI simple and clean.
13. UI/UX Polish: Now refine the look and usability:
- Style the table and headers, perhaps using a CSS library or custom styles for readability (e.g. alternating row colors, icons for up/down moves).
- Add formatting for numbers (large abbreviations like 1.2M for volume, etc., if not already done).
- Include the last updated timestamp in the UI and maybe a subtle auto-refresh indicator (like a spinning icon or changing timestamp).
- Test the responsive behavior on different screen sizes and fix any layout issues.
- If any interactive elements (like sorting columns or toggling between views) are planned, implement basic versions of those now.
14. Error Handling on Frontend: Simulate an API failure (e.g. point the frontend to a non-working endpoint or stop the backend) to ensure the frontend handles it gracefully. This might mean showing an error banner or keeping old data visible. Implement those user feedback elements as needed (e.g. use the last_updated to decide if data is too old and flag it).
15. Finalize Content & Documentation: Prepare the project for handoff or deployment. Write a concise README explaining how to deploy the backend and frontend. Double-check that configuration (like API base URLs, any keys) are properly set for production. Remove any console logs or debug code.
16. Deployment and Monitoring: Deploy the latest backend code (with volatility/correlation) to the serverless platform. Deploy the updated frontend. Verify end-to-end that the live site updates as expected. Set up basic monitoring or at least log checks to confirm scheduled jobs are running. At this point, the MVP should be fully functional and automated.
Phase 4: Post-MVP Enhancements (Deferred Features) – While not required for MVP, it’s worth noting steps for future improvements:
Implement additional pages (e.g. coin detail pages with charts) if desired.
Enhance caching or use a CDN for the API responses if scale becomes an issue.
Add tests (unit or integration tests for data fetching and calculations) to ensure reliability.
By following this build order, we start with a working simple dashboard early (Phase 2) and incrementally add the complex features like analytics and polish in Phase 3. This ensures that even if time is cut short, we have a functional product, and each step adds measurable value.
MVP Definition
The Minimum Viable Product (MVP) for the Crypto Market Dashboard is a functional web application that automatically displays key crypto market data without any manual updates. The MVP is considered “done” when the following are true:
Automated Data Updates: The system fetches live market data (prices, volumes, etc.) on a schedule and updates the display. No human intervention is needed after deployment – adding new data or refreshing is fully automated by the backend jobs.
Core Data Displayed: The dashboard shows the essential information:
Prices of major cryptocurrencies (e.g. at least the top 5–10 coins) in real-time or near-real-time.
24-hour change (percentage and/or absolute) for those coins.
Trading volume (24h) for those coins.
Market-wide totals like total market cap and total 24h volume.
These should be clearly visible on the main page in a logical layout (e.g. global metrics on top, coin list below).
Volatility & Correlation (MVP Scope): At least basic volatility and correlation metrics are included. For MVP, this could be:
A 30-day volatility percentage for the top coins (showing how much they fluctuate).
A correlation indicator (even if just one key correlation, such as each coin vs Bitcoin or a small matrix of top 3 coins).
These metrics need to be based on actual calculations from historical data (and updated periodically), demonstrating the site’s analytical capability beyond raw data.
No User Interaction Needed: The MVP dashboard should load and update on its own. There are no login, no user-specific settings, and no actions required by the user to see updated data. (A manual refresh button can exist, but the key is the data updates regardless.)
Tech Stack in Place: The application is deployed on a free or minimal-cost hosting: for example, the frontend is hosted on a static hosting service (like Netlify or GitHub Pages) and the backend is running on a serverless platform (like AWS Lambda or similar) within free-tier limits. This ensures the solution is practically runnable without additional cost.
Reliability Basics: The system can handle at least a modest load (a few concurrent users) and has fallbacks for data fetch failures (e.g. it will show last known data if new data can’t be fetched). There should be no major crashes or unhandled exceptions during normal operation.
Readability and Clarity: The UI is clean and easy to read. All displayed numbers have labels/units, and the page is not cluttered with extraneous information. It should be understandable in one glance what each figure means. (We avoid any hype language or distracting ads – it’s purely informational.)
Performance: The page loads reasonably fast (thanks to being static or cached) and API calls return quickly (since they hit our backend cache). Even on refresh intervals, the update should feel seamless. This is an indicator that our caching strategy works.
Anything beyond the above is considered deferred to post-MVP. Features and enhancements that are explicitly not required for v1 (MVP) but could be added later include:
Detailed Coin Pages or Charts: In MVP, we do not need separate pages with extensive historical charts or news for each coin. Those can be future enhancements once the core is stable.
User Accounts or Personalization: There will be no ability to log in, save portfolios, or set preferences in v1. MVP focuses on public data for all users. Personal watchlists, custom alerts, or portfolio tracking are out-of-scope to keep the project simple and avoid any handling of private user data or funds.
Real-Time Streaming Data: We are not implementing high-frequency updates like websocket live price ticks in MVP. The refresh every few minutes is sufficient. If needed, live streaming prices or order book data could be future work, but that would require different tech (and possibly handling many more updates per second).
Mobile App or Native Client: MVP is a web dashboard only. Any dedicated mobile app or integration (while possible through responsive design or PWA) is not considered in initial release.
Extensive Analytics: Beyond basic volatility and correlation, any complex analytics (like predictive indicators, on-chain metrics, sentiment analysis) are deferred. MVP sticks to straightforward market metrics.
Multi-Source Redundancy: Using multiple APIs for redundancy (to avoid reliance on one data source) is not in MVP. MVP will trust a single reliable API (CoinGecko). Later, one might add failover to another API if one goes down.
UI Polish Beyond Basics: While MVP includes a clean UI, any very advanced visualizations or elaborate design elements (animated charts, fancy graphics) are not necessary. The focus is on correctness and clarity; aesthetic improvements can come later as polish.
Scalability Concerns: The MVP is aimed at small to moderate usage. We don’t initially worry about scaling to millions of users or heavy optimizations. For instance, caching every 5 minutes is fine for MVP, even if that means data could be a few minutes behind. In future versions, if needed, we might refine the update frequency or scale the backend differently.
By meeting the MVP criteria above, we ensure the dashboard is functional, automated, and delivers the intended value (up-to-date crypto market insights). Once these are met, the product is ready to use, and additional features can be planned for subsequent versions. The team can confidently consider the MVP “done” when a user can visit the site and consistently see fresh crypto market data (prices, volumes, volatility, correlations, etc.) updated without any manual steps, all presented in a clear and digestible format .

